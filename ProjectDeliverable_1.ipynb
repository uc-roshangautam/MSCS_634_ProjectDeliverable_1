{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6de722",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import UCI ML repo, fallback to manual loading if needed\n",
    "try:\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    UCI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ucimlrepo not available. Will use manual dataset loading.\")\n",
    "    UCI_AVAILABLE = False\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Advanced Data Mining Project - Deliverable 1 ===\")\n",
    "print(\"Data Collection, Cleaning, and Exploration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA COLLECTION AND DATASET SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1. DATASET SELECTION AND JUSTIFICATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\"\"\"\n",
    "Dataset Selection: UCI Online Retail Dataset\n",
    "Source: UCI Machine Learning Repository (ID: 352)\n",
    "https://archive.ics.uci.edu/dataset/352/online+retail\n",
    "\n",
    "Dataset Description:\n",
    "This dataset contains transactional data from a UK-based non-store online retail company\n",
    "operating between 01/12/2010 and 09/12/2011. The company primarily sells unique \n",
    "all-occasion gifts with many wholesale customers.\n",
    "\n",
    "Dataset Characteristics:\n",
    "- Records: 541,909 transactions\n",
    "- Attributes: 8 features including customer, product, and transaction details\n",
    "- Time span: 11 months of retail operations\n",
    "- Geographic scope: Multiple countries with UK focus\n",
    "\n",
    "Justification for Dataset Selection:\n",
    "1. Size requirements: 541K+ records with 8 attributes exceeds project requirements\n",
    "2. Real-world business data: Authentic e-commerce transactions\n",
    "3. Rich analytical opportunities: Customer segmentation, sales forecasting, market basket analysis\n",
    "4. Multiple modeling possibilities: Supports regression, classification, clustering, and association rules\n",
    "5. Data quality challenges: Contains missing values and cancellations for cleaning practice\n",
    "6. Industry relevance: E-commerce analytics is highly valuable in current market\n",
    "\"\"\"\n",
    "\n",
    "# Load UCI Online Retail Dataset\n",
    "print(\"Loading UCI Online Retail Dataset...\")\n",
    "\n",
    "if UCI_AVAILABLE:\n",
    "    try:\n",
    "        online_retail = fetch_ucirepo(id=352)\n",
    "        df = online_retail.data.features.copy()\n",
    "        print(f\"Dataset loaded successfully from UCI repository\")\n",
    "    except Exception as e:\n",
    "        print(f\"UCI repository access failed: {e}\")\n",
    "        print(\"Please download the dataset manually from:\")\n",
    "        print(\"https://archive.ics.uci.edu/dataset/352/online+retail\")\n",
    "        print(\"Save as 'Online_Retail.xlsx' in the current directory\")\n",
    "        exit()\n",
    "else:\n",
    "    # Try to load from local file\n",
    "    try:\n",
    "        df = pd.read_excel('Online_Retail.xlsx')\n",
    "        print(\"Dataset loaded from local Excel file\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Dataset file not found. Please download from:\")\n",
    "        print(\"https://archive.ics.uci.edu/dataset/352/online+retail\")\n",
    "        print(\"Save as 'Online_Retail.xlsx' in the current directory\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "print(f\"Dataset contains {len(df)} records and {len(df.columns)} attributes\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: INITIAL DATA INSPECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2. INITIAL DATA INSPECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Check for negative quantities and cancellations\n",
    "print(f\"\\nTransactions with negative quantities: {(df['Quantity'] < 0).sum()}\")\n",
    "print(f\"Cancellation transactions (InvoiceNo starts with 'C'): {df['InvoiceNo'].astype(str).str.startswith('C').sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3. DATA CLEANING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Store original dataset info for comparison\n",
    "original_shape = df.shape\n",
    "print(f\"Original dataset shape: {original_shape}\")\n",
    "\n",
    "# 3.1 Handle Missing Values\n",
    "print(\"\\n3.1 Handling Missing Values:\")\n",
    "print(\"Missing values before cleaning:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# CustomerID has missing values - these represent transactions without customer identification\n",
    "missing_customers = df['CustomerID'].isnull().sum()\n",
    "print(f\"Transactions without CustomerID: {missing_customers} ({missing_customers/len(df)*100:.1f}%)\")\n",
    "\n",
    "# For customer-focused analysis, remove transactions without CustomerID\n",
    "df_with_customers = df.dropna(subset=['CustomerID']).copy()\n",
    "print(f\"Dataset after removing missing CustomerID: {df_with_customers.shape}\")\n",
    "\n",
    "# Check for missing descriptions\n",
    "missing_descriptions = df_with_customers['Description'].isnull().sum()\n",
    "if missing_descriptions > 0:\n",
    "    print(f\"Transactions with missing Description: {missing_descriptions}\")\n",
    "    df_with_customers = df_with_customers.dropna(subset=['Description'])\n",
    "    print(f\"Dataset after removing missing descriptions: {df_with_customers.shape}\")\n",
    "\n",
    "# 3.2 Handle Cancellations and Returns\n",
    "print(\"\\n3.2 Handling Cancellations and Returns:\")\n",
    "cancellations = df_with_customers['InvoiceNo'].astype(str).str.startswith('C')\n",
    "print(f\"Cancellation transactions: {cancellations.sum()}\")\n",
    "\n",
    "# Separate cancellations for analysis but keep for now\n",
    "df_with_customers['is_cancellation'] = cancellations\n",
    "\n",
    "# Handle negative quantities\n",
    "negative_qty = df_with_customers['Quantity'] < 0\n",
    "print(f\"Transactions with negative quantities: {negative_qty.sum()}\")\n",
    "\n",
    "# For initial analysis, focus on positive transactions\n",
    "df_positive = df_with_customers[df_with_customers['Quantity'] > 0].copy()\n",
    "print(f\"Dataset with positive quantities only: {df_positive.shape}\")\n",
    "\n",
    "# 3.3 Handle Data Inconsistencies\n",
    "print(\"\\n3.3 Handling Data Inconsistencies:\")\n",
    "\n",
    "# Check for zero unit prices\n",
    "zero_prices = (df_positive['UnitPrice'] <= 0).sum()\n",
    "print(f\"Transactions with zero or negative unit prices: {zero_prices}\")\n",
    "\n",
    "if zero_prices > 0:\n",
    "    df_positive = df_positive[df_positive['UnitPrice'] > 0]\n",
    "    print(f\"Dataset after removing zero prices: {df_positive.shape}\")\n",
    "\n",
    "# Clean description field\n",
    "df_positive['Description'] = df_positive['Description'].str.strip().str.upper()\n",
    "\n",
    "# Convert data types\n",
    "df_positive['CustomerID'] = df_positive['CustomerID'].astype(int)\n",
    "df_positive['InvoiceDate'] = pd.to_datetime(df_positive['InvoiceDate'])\n",
    "\n",
    "# 3.4 Feature Engineering\n",
    "print(\"\\n3.4 Feature Engineering:\")\n",
    "\n",
    "# Create total amount per transaction line\n",
    "df_positive['TotalAmount'] = df_positive['Quantity'] * df_positive['UnitPrice']\n",
    "\n",
    "# Extract date components\n",
    "df_positive['Year'] = df_positive['InvoiceDate'].dt.year\n",
    "df_positive['Month'] = df_positive['InvoiceDate'].dt.month\n",
    "df_positive['DayOfWeek'] = df_positive['InvoiceDate'].dt.dayofweek\n",
    "df_positive['Hour'] = df_positive['InvoiceDate'].dt.hour\n",
    "\n",
    "print(f\"Added TotalAmount and temporal features\")\n",
    "\n",
    "# 3.5 Handle Outliers\n",
    "print(\"\\n3.5 Outlier Detection and Treatment:\")\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in key numerical columns\n",
    "numerical_cols = ['Quantity', 'UnitPrice', 'TotalAmount']\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_positive, col)\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': len(outliers) / len(df_positive) * 100,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(df_positive)*100:.1f}%)\")\n",
    "\n",
    "# Cap extreme outliers for UnitPrice and TotalAmount\n",
    "df_cleaned = df_positive.copy()\n",
    "for col in ['UnitPrice', 'TotalAmount']:\n",
    "    Q99 = df_cleaned[col].quantile(0.99)\n",
    "    extreme_outliers = df_cleaned[col] > Q99\n",
    "    print(f\"Capping {extreme_outliers.sum()} extreme outliers in {col} at {Q99:.2f}\")\n",
    "    df_cleaned.loc[df_cleaned[col] > Q99, col] = Q99\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Data cleaning summary:\")\n",
    "print(f\"- Started with {original_shape[0]} transactions\")\n",
    "print(f\"- Removed {original_shape[0] - len(df_cleaned)} problematic records\")\n",
    "print(f\"- Final dataset: {len(df_cleaned)} valid transactions\")\n",
    "\n",
    "# Update main dataframe\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4.1 Temporal Analysis\n",
    "print(\"\\n4.1 Temporal Analysis\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Monthly sales trend\n",
    "plt.subplot(2, 3, 1)\n",
    "monthly_sales = df.groupby(['Year', 'Month'])['TotalAmount'].sum().reset_index()\n",
    "monthly_sales['YearMonth'] = monthly_sales['Year'].astype(str) + '-' + monthly_sales['Month'].astype(str).str.zfill(2)\n",
    "plt.plot(range(len(monthly_sales)), monthly_sales['TotalAmount'])\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales Amount')\n",
    "plt.xticks(range(0, len(monthly_sales), 2), monthly_sales['YearMonth'][::2], rotation=45)\n",
    "\n",
    "# Daily transaction pattern\n",
    "plt.subplot(2, 3, 2)\n",
    "daily_pattern = df.groupby('DayOfWeek')['InvoiceNo'].nunique()\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plt.bar(days, daily_pattern.values)\n",
    "plt.title('Transactions by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# Hourly transaction pattern\n",
    "plt.subplot(2, 3, 3)\n",
    "hourly_pattern = df.groupby('Hour')['InvoiceNo'].nunique()\n",
    "plt.bar(hourly_pattern.index, hourly_pattern.values)\n",
    "plt.title('Transactions by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# Quantity distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(df['Quantity'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Quantity Distribution')\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, df['Quantity'].quantile(0.95))\n",
    "\n",
    "# Unit price distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(df['UnitPrice'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Unit Price Distribution')\n",
    "plt.xlabel('Unit Price (GBP)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, df['UnitPrice'].quantile(0.95))\n",
    "\n",
    "# Total amount distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(df['TotalAmount'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Total Amount Distribution')\n",
    "plt.xlabel('Total Amount (GBP)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, df['TotalAmount'].quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.2 Customer Analysis\n",
    "print(\"\\n4.2 Customer Analysis\")\n",
    "\n",
    "# Customer metrics\n",
    "customer_metrics = df.groupby('CustomerID').agg({\n",
    "    'InvoiceNo': 'nunique',  # Number of orders\n",
    "    'Quantity': 'sum',       # Total items purchased\n",
    "    'TotalAmount': 'sum',    # Total spent\n",
    "    'InvoiceDate': ['min', 'max']  # First and last purchase\n",
    "}).round(2)\n",
    "\n",
    "customer_metrics.columns = ['OrderCount', 'TotalQuantity', 'TotalSpent', 'FirstPurchase', 'LastPurchase']\n",
    "customer_metrics['CustomerLifespan'] = (customer_metrics['LastPurchase'] - customer_metrics['FirstPurchase']).dt.days\n",
    "customer_metrics['AvgOrderValue'] = customer_metrics['TotalSpent'] / customer_metrics['OrderCount']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Customer order count distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(customer_metrics['OrderCount'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Customer Order Count Distribution')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Customer total spent distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(customer_metrics['TotalSpent'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Customer Total Spending Distribution')\n",
    "plt.xlabel('Total Spent (GBP)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlim(0, customer_metrics['TotalSpent'].quantile(0.95))\n",
    "\n",
    "# Average order value distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(customer_metrics['AvgOrderValue'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Average Order Value Distribution')\n",
    "plt.xlabel('Average Order Value (GBP)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlim(0, customer_metrics['AvgOrderValue'].quantile(0.95))\n",
    "\n",
    "# Customer lifespan distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(customer_metrics['CustomerLifespan'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Customer Lifespan Distribution')\n",
    "plt.xlabel('Days Between First and Last Purchase')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Top countries by sales\n",
    "plt.subplot(2, 3, 5)\n",
    "country_sales = df.groupby('Country')['TotalAmount'].sum().sort_values(ascending=False).head(10)\n",
    "plt.bar(range(len(country_sales)), country_sales.values)\n",
    "plt.title('Top 10 Countries by Sales')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Total Sales (GBP)')\n",
    "plt.xticks(range(len(country_sales)), country_sales.index, rotation=45)\n",
    "\n",
    "# RFM Analysis preparation\n",
    "reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "rfm = df.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (reference_date - x.max()).days,\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalAmount': 'sum'\n",
    "}).round(2)\n",
    "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(rfm['Recency'], rfm['Monetary'], alpha=0.6)\n",
    "plt.title('RFM Analysis: Recency vs Monetary')\n",
    "plt.xlabel('Recency (Days)')\n",
    "plt.ylabel('Monetary Value (GBP)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.3 Product Analysis\n",
    "print(\"\\n4.3 Product Analysis\")\n",
    "\n",
    "# Product metrics\n",
    "product_metrics = df.groupby('StockCode').agg({\n",
    "    'Description': 'first',\n",
    "    'Quantity': 'sum',\n",
    "    'TotalAmount': 'sum',\n",
    "    'CustomerID': 'nunique'\n",
    "}).round(2)\n",
    "product_metrics.columns = ['Description', 'TotalQuantitySold', 'TotalRevenue', 'UniqueCustomers']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Top products by quantity\n",
    "plt.subplot(2, 2, 1)\n",
    "top_qty_products = product_metrics.nlargest(10, 'TotalQuantitySold')\n",
    "plt.barh(range(len(top_qty_products)), top_qty_products['TotalQuantitySold'])\n",
    "plt.title('Top 10 Products by Quantity Sold')\n",
    "plt.xlabel('Total Quantity Sold')\n",
    "plt.yticks(range(len(top_qty_products)), top_qty_products['Description'].str[:30])\n",
    "\n",
    "# Top products by revenue\n",
    "plt.subplot(2, 2, 2)\n",
    "top_revenue_products = product_metrics.nlargest(10, 'TotalRevenue')\n",
    "plt.barh(range(len(top_revenue_products)), top_revenue_products['TotalRevenue'])\n",
    "plt.title('Top 10 Products by Revenue')\n",
    "plt.xlabel('Total Revenue (GBP)')\n",
    "plt.yticks(range(len(top_revenue_products)), top_revenue_products['Description'].str[:30])\n",
    "\n",
    "# Product popularity (unique customers)\n",
    "plt.subplot(2, 2, 3)\n",
    "top_popular_products = product_metrics.nlargest(10, 'UniqueCustomers')\n",
    "plt.barh(range(len(top_popular_products)), top_popular_products['UniqueCustomers'])\n",
    "plt.title('Top 10 Products by Customer Reach')\n",
    "plt.xlabel('Number of Unique Customers')\n",
    "plt.yticks(range(len(top_popular_products)), top_popular_products['Description'].str[:30])\n",
    "\n",
    "# Price vs Quantity relationship\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(df['UnitPrice'], df['Quantity'], alpha=0.6)\n",
    "plt.title('Unit Price vs Quantity Relationship')\n",
    "plt.xlabel('Unit Price (GBP)')\n",
    "plt.ylabel('Quantity')\n",
    "plt.xlim(0, df['UnitPrice'].quantile(0.95))\n",
    "plt.ylim(0, df['Quantity'].quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.4 Correlation Analysis\n",
    "print(\"\\n4.4 Correlation Analysis\")\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_df = df[['Quantity', 'UnitPrice', 'TotalAmount', 'Year', 'Month', 'DayOfWeek', 'Hour']].copy()\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key correlations found:\")\n",
    "strong_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.3:\n",
    "            strong_corr.append((correlation_matrix.columns[i], \n",
    "                              correlation_matrix.columns[j], corr_val))\n",
    "\n",
    "for var1, var2, corr in strong_corr:\n",
    "    print(f\"- {var1} vs {var2}: {corr:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: KEY INSIGHTS AND SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS AND SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n5.1 Dataset Summary:\")\n",
    "print(f\"- Final dataset contains {len(df)} transaction records\")\n",
    "print(f\"- Covers {df['CustomerID'].nunique()} unique customers\")\n",
    "print(f\"- Includes {df['StockCode'].nunique()} unique products\")\n",
    "print(f\"- Spans {df['Country'].nunique()} countries\")\n",
    "print(f\"- Time period: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
    "\n",
    "print(\"\\n5.2 Business Performance Insights:\")\n",
    "total_revenue = df['TotalAmount'].sum()\n",
    "avg_order_value = df.groupby('InvoiceNo')['TotalAmount'].sum().mean()\n",
    "total_transactions = df['InvoiceNo'].nunique()\n",
    "\n",
    "print(f\"- Total revenue: £{total_revenue:,.2f}\")\n",
    "print(f\"- Total transactions: {total_transactions:,}\")\n",
    "print(f\"- Average order value: £{avg_order_value:.2f}\")\n",
    "print(f\"- Average items per transaction: {df.groupby('InvoiceNo')['Quantity'].sum().mean():.1f}\")\n",
    "\n",
    "print(\"\\n5.3 Customer Behavior Insights:\")\n",
    "print(f\"- Average customer lifetime: {customer_metrics['CustomerLifespan'].mean():.0f} days\")\n",
    "print(f\"- Average orders per customer: {customer_metrics['OrderCount'].mean():.1f}\")\n",
    "print(f\"- Average customer value: £{customer_metrics['TotalSpent'].mean():.2f}\")\n",
    "print(f\"- Most active country: {df.groupby('Country')['TotalAmount'].sum().idxmax()}\")\n",
    "\n",
    "print(\"\\n5.4 Product Performance Insights:\")\n",
    "best_selling_product = product_metrics.loc[product_metrics['TotalQuantitySold'].idxmax(), 'Description']\n",
    "highest_revenue_product = product_metrics.loc[product_metrics['TotalRevenue'].idxmax(), 'Description']\n",
    "\n",
    "print(f\"- Best-selling product by quantity: {best_selling_product}\")\n",
    "print(f\"- Highest revenue product: {highest_revenue_product}\")\n",
    "print(f\"- Average product price: £{df['UnitPrice'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n5.5 Temporal Patterns:\")\n",
    "peak_month = monthly_sales.loc[monthly_sales['TotalAmount'].idxmax(), 'YearMonth']\n",
    "peak_day = days[daily_pattern.idxmax()]\n",
    "peak_hour = hourly_pattern.idxmax()\n",
    "\n",
    "print(f\"- Peak sales month: {peak_month}\")\n",
    "print(f\"- Most active day: {peak_day}\")\n",
    "print(f\"- Peak transaction hour: {peak_hour}:00\")\n",
    "print(f\"- Sales seasonality: Strong November/December peak visible\")\n",
    "\n",
    "print(\"\\n5.6 Data Quality Assessment:\")\n",
    "print(f\"- Data completeness: {(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.1f}%\")\n",
    "print(f\"- Customer identification rate: {len(df) / original_shape[0] * 100:.1f}%\")\n",
    "print(f\"- Transaction value validity: 100% (after cleaning)\")\n",
    "\n",
    "print(\"\\n5.7 Implications for Future Modeling:\")\n",
    "print(\"- Customer Segmentation: Clear RFM patterns for customer classification\")\n",
    "print(\"- Sales Forecasting: Strong temporal patterns for time series modeling\")\n",
    "print(\"- Market Basket Analysis: Rich product transaction data available\")\n",
    "print(\"- Churn Prediction: Customer purchase frequency and recency patterns\")\n",
    "print(\"- Revenue Optimization: Price-quantity relationships identified\")\n",
    "print(\"- Geographic Analysis: Multi-country sales patterns for expansion modeling\")\n",
    "\n",
    "print(\"\\n5.8 Recommended Feature Engineering for Next Deliverables:\")\n",
    "print(\"- Customer lifetime value calculations\")\n",
    "print(\"- RFM score creation for segmentation\")\n",
    "print(\"- Product category clustering\")\n",
    "print(\"- Seasonal adjustment factors\")\n",
    "print(\"- Customer churn indicators\")\n",
    "print(\"- Market basket association metrics\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL DATASET EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. DATASET EXPORT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_filename = 'cleaned_online_retail_dataset.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"Cleaned dataset exported to: {output_filename}\")\n",
    "\n",
    "# Save customer metrics for future use\n",
    "customer_metrics.to_csv('customer_metrics.csv')\n",
    "print(f\"Customer metrics exported to: customer_metrics.csv\")\n",
    "\n",
    "# Save RFM analysis\n",
    "rfm.to_csv('rfm_analysis.csv')\n",
    "print(f\"RFM analysis exported to: rfm_analysis.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
